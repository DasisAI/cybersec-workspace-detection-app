{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786ee807-26d8-49db-95c2-15d697879dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install geoip2 netaddr\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4234b5e0-bc6b-493e-8dd5-d6c2f75005d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import builtins\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "common = dbutils.import_notebook(\"lib.common\")  # repo의 /lib/common.py\n",
    "builtins.detect = common.detect                # @detect 데코레이터를 룰 노트북이 찾게 함\n",
    "builtins.Output = common.Output\n",
    "builtins.dbutils = dbutils\n",
    "builtins.spark = spark\n",
    "builtins.F = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1879314d-3087-4b94-8389-d2e1b1a918b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 룰 노트북들이 import 시점에 dbutils.widgets.get(\"earliest\"/\"latest\")를 호출하므로\n",
    "# Runner에서 위젯을 먼저 만들어둔다.\n",
    "\n",
    "for k in [\"earliest\", \"latest\"]:\n",
    "    try:\n",
    "        dbutils.widgets.remove(k)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dbutils.widgets.text(\"earliest\", \"\") # 예시 2026-02-01 00:00:00\n",
    "dbutils.widgets.text(\"latest\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29e8978-76ff-4fa9-985c-8ca23449797d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "nb_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_ws_root = \"/\".join(nb_path.split(\"/\")[:4])          # /Repos/<user>/<repo>\n",
    "repo_fs_root = f\"/Workspace{repo_ws_root}\"               # /Workspace/Repos/<user>/<repo>\n",
    "materialized_fs_root = f\"{repo_fs_root}/materialized_py\" # /Workspace/Repos/<user>/<repo>/materialized_py\n",
    "\n",
    "if materialized_fs_root not in sys.path:\n",
    "    sys.path.insert(0, materialized_fs_root)\n",
    "\n",
    "print(\"added_to_sys.path =\", materialized_fs_root)\n",
    "print(\"sys.path[0:3] =\", sys.path[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86dc3875-07b8-497b-a1cf-a4becd363dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks 위젯(파라미터)\n",
    "dbutils.widgets.text(\"rule_group\", \"binary\")      # binary | behavioral\n",
    "dbutils.widgets.text(\"rule_ids\", \"\")              # 옵션: 콤마로 rule_id 지정 (비우면 전체)\n",
    "dbutils.widgets.dropdown(\"dry_run\", \"false\", [\"false\", \"true\"])  # true면 저장 skip\n",
    "\n",
    "dry_run = dbutils.widgets.get(\"dry_run\").strip().lower() == \"true\"\n",
    "rule_group = dbutils.widgets.get(\"rule_group\").strip()\n",
    "rule_ids_raw = dbutils.widgets.get(\"rule_ids\").strip()\n",
    "rule_ids = [x.strip() for x in rule_ids_raw.split(\",\") if x.strip()] if rule_ids_raw else []\n",
    "\n",
    "# 룰 로드\n",
    "query = f\"\"\"\n",
    "SELECT rule_id, rule_group, module_path, callable_name, lookback_minutes\n",
    "FROM sandbox.audit_poc.rule_registry\n",
    "WHERE enabled = true\n",
    "  AND rule_group = '{rule_group}'\n",
    "\"\"\"\n",
    "\n",
    "if rule_ids:\n",
    "    ids = \",\".join([f\"'{x}'\" for x in rule_ids])\n",
    "    query += f\" AND rule_id IN ({ids})\"\n",
    "\n",
    "rules_df = spark.sql(query)\n",
    "\n",
    "display(rules_df)\n",
    "print(\"loaded_rules_count =\", rules_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e834fbf-2f2c-42fb-a0fa-cd3e3e213ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, expr, coalesce\n",
    "\n",
    "# checkpoint 조인해서 window 계산\n",
    "# earliest = last_success_end_ts가 있으면 그 값, 없으면 now - lookback_minutes\n",
    "# latest   = now\n",
    "windows_df = (\n",
    "    rules_df.alias(\"r\")\n",
    "    .join(\n",
    "        spark.table(\"sandbox.audit_poc.rule_checkpoint\").alias(\"c\"),\n",
    "        on=\"rule_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"r.rule_id\"),\n",
    "        col(\"r.rule_group\"),\n",
    "        col(\"r.module_path\"),\n",
    "        col(\"r.callable_name\"),\n",
    "        col(\"r.lookback_minutes\"),\n",
    "        coalesce(\n",
    "            col(\"c.last_success_end_ts\"),\n",
    "            expr(\"current_timestamp() - INTERVAL 1 MINUTE * lookback_minutes\")\n",
    "        ).alias(\"window_start_ts\"),\n",
    "        current_timestamp().alias(\"window_end_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(windows_df.select(\"rule_id\",\"window_start_ts\",\"window_end_ts\",\"lookback_minutes\").orderBy(\"rule_id\"))\n",
    "print(\"windows_count =\", windows_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095870ba-fc8d-4411-a26d-0ef03d945a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nb_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_ws_root = \"/\".join(nb_path.split(\"/\")[:4])  # /Repos/<user>/<repo>\n",
    "\n",
    "detections_ws_root = f\"{repo_ws_root}/base/detections\"\n",
    "detections_fs_root = f\"/Workspace{detections_ws_root}\"\n",
    "rule_groups = [\n",
    "    d for d in sorted(os.listdir(detections_fs_root))\n",
    "    if os.path.isdir(os.path.join(detections_fs_root, d)) and not d.startswith((\"_\", \".\"))\n",
    "]\n",
    "\n",
    "updates = []\n",
    "for rg in rule_groups:\n",
    "    folder_ws = f\"{detections_ws_root}/{rg}\"\n",
    "    folder_fs = f\"{detections_fs_root}/{rg}\"\n",
    "\n",
    "    for fname in sorted(os.listdir(folder_fs)):\n",
    "        if not fname.endswith(\".py\") or fname.startswith(\"_\"):\n",
    "            continue\n",
    "        rule_id = fname[:-3]\n",
    "        notebook_path = f\"{folder_ws}/{fname}\"\n",
    "        updates.append((rule_id, notebook_path, rg))\n",
    "\n",
    "df_upd = spark.createDataFrame(updates, \"rule_id string, notebook_path string, rule_group string\")\n",
    "df_upd.createOrReplaceTempView(\"upd\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO sandbox.audit_poc.rule_registry t\n",
    "USING upd s\n",
    "ON t.rule_id = s.rule_id\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  t.notebook_path = s.notebook_path,\n",
    "  t.rule_group = COALESCE(t.rule_group, s.rule_group),\n",
    "  t.run_mode = 'NOTEBOOK',\n",
    "  t.updated_at = current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"SELECT rule_group, COUNT(*) cnt, COUNT(notebook_path) nb_cnt FROM sandbox.audit_poc.rule_registry GROUP BY rule_group ORDER BY rule_group\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b934fa-37fe-4a96-825b-c726ba44123e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 통합 테이블 적재함수 \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "UNIFIED_TBL = \"sandbox.audit_poc.findings_unified\"\n",
    "\n",
    "_EXCLUDE_FROM_PAYLOAD = {\n",
    "    \"observed_at\", \"rule_id\", \"run_id\", \"window_start_ts\", \"window_end_ts\", \"dedupe_key\"\n",
    "}\n",
    "\n",
    "def _resolve_event_ts_col(df):\n",
    "    for c in [\"EVENT_TIME\", \"event_time\", \"event_ts\", \"EVENT_TS\", \"timestamp\", \"time\"]:\n",
    "        if c in df.columns:\n",
    "            return F.col(c).cast(\"timestamp\")\n",
    "    return F.current_timestamp()\n",
    "\n",
    "def _payload_struct(df):\n",
    "    cols = [c for c in df.columns if c not in _EXCLUDE_FROM_PAYLOAD]\n",
    "    if not cols:\n",
    "        return F.lit(\"{}\")\n",
    "    return F.to_json(F.struct(*[F.col(c) for c in cols]))\n",
    "\n",
    "def _write_unified(rule_id: str, df, run_id: str, start_ts, end_ts) -> None:\n",
    "    event_ts_expr = _resolve_event_ts_col(df)\n",
    "    payload_json_expr = _payload_struct(df)\n",
    "\n",
    "    unified_df = (\n",
    "        df.select(\n",
    "            event_ts_expr.alias(\"event_ts\"),\n",
    "            F.to_date(event_ts_expr).alias(\"event_date\"),\n",
    "            F.lit(rule_id).alias(\"rule_id\"),\n",
    "            F.lit(run_id).alias(\"run_id\"),\n",
    "            F.lit(start_ts).alias(\"window_start_ts\"),\n",
    "            F.lit(end_ts).alias(\"window_end_ts\"),\n",
    "            F.current_timestamp().alias(\"observed_at\"),\n",
    "            payload_json_expr.alias(\"payload_json\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"dedupe_key\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\n",
    "                    \"||\",\n",
    "                    F.col(\"rule_id\"),\n",
    "                    F.coalesce(F.col(\"event_ts\").cast(\"string\"), F.lit(\"\")),\n",
    "                    F.col(\"payload_json\"),\n",
    "                ),\n",
    "                256,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    (unified_df.write.format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(UNIFIED_TBL)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc51aaf-77b3-42dc-be8d-51497763375d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, importlib\n",
    "from pyspark.sql.functions import current_timestamp, lit, sha2, concat_ws\n",
    "\n",
    "\n",
    "run_id = f\"{rule_group}_{int(time.time())}\"\n",
    "\n",
    "selected_rules = windows_df.filter(f\"rule_group = '{rule_group}'\").collect()\n",
    "\n",
    "results = []\n",
    "\n",
    "_EXCLUDE_FROM_DEDUPE = {\"observed_at\", \"run_id\", \"window_start_ts\", \"window_end_ts\"}\n",
    "\n",
    "for r in selected_rules:\n",
    "    rule_id = r[\"rule_id\"]\n",
    "    start_ts = r[\"window_start_ts\"]\n",
    "    end_ts = r[\"window_end_ts\"]\n",
    "\n",
    "    # registry에서 module_path/callable_name 재조회(최신값 보장)\n",
    "    meta = spark.sql(f\"\"\"\n",
    "    SELECT module_path, callable_name\n",
    "    FROM sandbox.audit_poc.rule_registry\n",
    "    WHERE rule_id = '{rule_id}' AND enabled = true\n",
    "    \"\"\").collect()\n",
    "    if not meta:\n",
    "        continue\n",
    "    module_path, callable_name = meta[0][\"module_path\"], meta[0][\"callable_name\"]\n",
    "\n",
    "    t0 = time.time()\n",
    "    status = \"SUCCESS\"\n",
    "    err = None\n",
    "    row_count = None\n",
    "\n",
    "    try:\n",
    "        mod = dbutils.import_notebook(module_path)\n",
    "        fn = getattr(mod, callable_name)\n",
    "        df = fn(earliest=str(start_ts), latest=str(end_ts))\n",
    "\n",
    "        row_count = df.count()\n",
    "\n",
    "        out_df = (\n",
    "            df\n",
    "            .withColumn(\"observed_at\", current_timestamp())\n",
    "            .withColumn(\"rule_id\", lit(rule_id))\n",
    "            .withColumn(\"run_id\", lit(run_id))\n",
    "            .withColumn(\"window_start_ts\", lit(start_ts))\n",
    "            .withColumn(\"window_end_ts\", lit(end_ts))\n",
    "        )\n",
    "\n",
    "        base_cols = [c for c in out_df.columns if c not in _EXCLUDE_FROM_DEDUPE]\n",
    "        if \"rule_id\" not in base_cols:\n",
    "            base_cols = [\"rule_id\"] + base_cols\n",
    "\n",
    "        out_df = out_df.withColumn(\n",
    "            \"dedupe_key\",\n",
    "            sha2(concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in base_cols]), 256)\n",
    "        )\n",
    "        if rule_id == 'user_account_created':\n",
    "            print(rule_id)\n",
    "            print(out_df.select(concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in base_cols]).alias(\"value\")).toPandas()[\"value\"].tolist())\n",
    "            print(out_df.select(sha2(concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in base_cols]),256).alias(\"value\")).toPandas()[\"value\"].tolist())\n",
    "\n",
    "        target_table = f\"sandbox.audit_poc.findings_{rule_id}\"\n",
    "\n",
    "        if not dry_run:\n",
    "            (out_df.write.format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .saveAsTable(target_table)\n",
    "            )\n",
    "\n",
    "            # 통합 테이블 저장\n",
    "            _write_unified(rule_id, df, run_id, start_ts, end_ts)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILED\"\n",
    "        err = str(e)[:4000]\n",
    "\n",
    "    duration_ms = int((time.time() - t0) * 1000)\n",
    "    results.append((run_id, rule_id, str(start_ts), str(end_ts), status, row_count, duration_ms, err))\n",
    "\n",
    "summary_df = spark.createDataFrame(\n",
    "    results,\n",
    "    \"run_id string, rule_id string, window_start string, window_end string, status string, row_count long, duration_ms long, error string\"\n",
    ")\n",
    "display(summary_df.orderBy(\"status\", \"rule_id\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_runner",
   "widgets": {
    "dry_run": {
     "currentValue": "true",
     "nuid": "9209e5c9-51ce-425e-9179-08a301c8edec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "dry_run",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "false",
        "true"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "dry_run",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "false",
        "true"
       ]
      }
     }
    },
    "earliest": {
     "currentValue": "",
     "nuid": "52f96e23-d06f-444a-9722-51b2628e48a0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "earliest",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "earliest",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "latest": {
     "currentValue": "",
     "nuid": "197cbdcd-7f92-426f-a04a-b8994d28d20c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "latest",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "latest",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rule_group": {
     "currentValue": "behavioral",
     "nuid": "a6625603-37ed-43f9-8847-9b83922d86d8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "binary",
      "label": null,
      "name": "rule_group",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "binary",
      "label": null,
      "name": "rule_group",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rule_ids": {
     "currentValue": "",
     "nuid": "f3743364-6788-43bb-bb3c-fc6f4f94600f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "rule_ids",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "rule_ids",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
